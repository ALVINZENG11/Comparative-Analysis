# A Comparative Analysis of Pretrained Summarization Models: Exploring Architecture, Tokenizers, and Evaluation Metrics.
## Objective 
Comparing the performance of two state-of-the-art pretrained summarization models:
* Main Focus since they're known for text summarization: PEGASUS, BART (T5, FLAN-T5, or Longformer Encoder-Decoder, llama 3 and 4 and gpt-4o and Claude basically anthropic or Open AI APIs), focusing comparisons on the representations generated by the models and to explore the distributional differences in how the architectures may have learned different dependencies on input, how they autoregressive generate, how they attend to context, etc. I am conducting an analysis on ground truth vs inferences and focusing on why the two models differ (architectural inductive biases, pre-training objectives) and how those differences manifest in faithfulness, coherence, and generalization. Pair automated metrics with human evaluation and behavioral analysis for a holistic view.   

**Reason** For choosing Pegasus and BART to start is because they are both free, open source models available via Hugging Face and neither requires API access. If times allow, I will also do an analysis on Anthropic 3.7 vs OpenAI o1 but both will require money and API access.   

Focusing this analysis on the CNN/DailyMail and XSum datasets in terms of summarization quality, generalization, and efficiency classifying all articles into worth length of "high", "medium", and "low". Some questions I am thinking about while conducting this Analysis is: 
* Is it this architecture with these parameters? 
* Or is it that architecture with this tokenizer? 
* What types of sentences or stories do model two which are properly summarized by model one not properly summarized? 

## DataSets
CNN/DailyMail: Longer summaries, closer to extractive style.   
* paired examples containing: article title (the full news story), summary   

XSum: One-sentence abstractive summaries with more compression and paraphrasing.   
* contains document (full news article text) and summary
Comparing model behavior across these datasets highlights strengths and weaknesses.   

## Evaluation Criteria
ROUGE-1/2/L: Standard n-gram overlap. Rouge (Recall-Oriented Understudy for Gisting Evaluation) metrics has become standard for evaluating automatic summarization systems.
* Rouge1 measures the overlap of individual words (unigrams) between generated and reference summary. (Helps us understand if our model is capturing key terms and concepts)
    * High ROUGE-1 scores suggest that the model is selecting the right information to include, but don't necessarily indicate good sentence structure or fluency.
* Rouge2 measures the overlap of adjacent word pairs (bigrams) between generated and reference summary. (How the model's ability to maintain proper phrasing and coherence)
    * Higher ROUGE-2 scores typically indicate that the summary not only contains the right information but also presents it with similar phrasing to the reference.
* RougeL measures longest common subsequence (LCS) between generated and reference summary. (tells us how well the generated text maintains the overall flow of information.)
    * Higher ROUGE-L scores suggest that the generated summary maintains a similar sequence of information to the reference summary, even if there are some different words used in between.   

The reason why I used Rouge to measure initially is because it is the standard metric for summarization evaluation largely because it is computationally efficient to calculate and relatively easy to interpret to name a few. Despite its ease of use, it primarily measures lexical overlap between generated and reference summaries, which misses many important aspects of summary quality.   

BERTScore: Semantic similarity using contextual embeddings. This solves one of the limitations of Rouge score by measuring semantic similarity rather than just lexical overlap. It uses contextual embeddings from BERT to compute similarity between generated and reference summaries.   
* BERTScore captures when a model expresses the same meaning using different words - crucial for abstractive summarization that Rouge often penalizes.   

SummaC: A factuality metric that combines entailment and contradiction detection. In other words, it measures the factual consistency between a summary and its source document. It evaluates directly whether each claim in the generated summary (inference) is supported by the source text (inputted article). This is important because the model can have a high BERTScore or Rouge score by containing the right key words but still make factually incorrect statements and does not depend on the ground truth (reference summary). This operates on entailment-based approach:
1. Divides the summary into sentence level claims
2. For each summary, calculates an entailment score against each other sentence in the source document
3. Aggregates these scores to determine if the summary sentence is supported by any part of the document
4. Combines all sentence-level scores into an overall factual consistency score   

Two different variants: SummaC-ZS (zero shot version that aggregates by using max and mean operators to combine sentence level entailment scores) and SummaC-Conv (More optimized version that uses a convolutional layer for aggregation or trained model with a single layer of convolution layer that aggregates the distribution of entailment scores into a single score). Higher scores would mean fewer hallucinations.

Length ratios / compression rates: To assess how much info is compressed. Ideal ratio for CNN/Dailymail is ~5-10x. Or if an article has 800 words, we would expect the summary to be 80-160 words. The lower the length deviation, the better. In our case, the inferences generated are ~60 words which is a bit shorter than what we expect.   
Human Evaluation:
* Fluency: Is it grammatically sound?
* Relevance: Are key points preserved?
* Factual consistency (faithfulness): Does the summary hallucinate?
    * Higher levels of abstract evaluation like factuality, hallucination

## Analysis: Pegasus and BART have different architectures; BART uses standard attention while Pegasus has sparse attention.
* BART might be faster on shorter texts (and struggles with very long texts)
    * Tokenizer SentencePiece
* Pegasus might be faster on longers one (handle longer texts more efficiently but slower inference) 
    * Denoising Transformer   

### Part 1: CNN/DailyMail Dataset
Import Pegasus and BART models that are fine-tuned on CNN/DailyMail.  These models are already specialized for summarization on CNN/DailyMail. Deciding between using pipeline utility and AutoModelForSeq2SeqLM/AutoTokenizer is that pipeline is easier to use due to its simpler API, automatic model/tokenizer loading and predefined arguments for common tasks but it is less flexible and harder to debug. AutoModel and AutoTokenizer allow full customization and are better for research projects. Since I am comparing architectures of Pegasus vs BART (analyzing attention/representations) I decided to use AutoModel/AutoTokenizer. Since models generate new summaries (inferences) take a long time, this is the first step I will be doing. I will be storing it as both a pickle and csv file since the notebook may crash or have other technical difficulties. CSV files are generally more readable and user friendly while pickle files allow us to preserve python object structures, handle special characters and newlines in text better, typically faster to load and save for large datasets, and maintain python data types correctly. Either way, I will be using both.   

After processing my models and creating the inferences for both PEGASUS and BART using the CNN/DailyMail Dataset, I evaluated them on the metrics explained above. My main motive is to understand which model is better at summarizing so I compared both the normal version and their pretrained on the dataset version. Both showed similar scores so I will be focusing on the already pretrained models. (This process did take quite some time and computing for BART was generally faster than pegasus) For each article in my dataset, I will have one ground truth summary (which comes from the dataset I'm using) and for each article, I will have two candidate summaries (one generated by Pegasus and one by BART).   

* Rouge Scores: For each model:
    * Pegasus (pretrained) - rouge1: 0.341, rouge2: 0.138, rougeL: 0.253
    * Pegasus (normal) - rouge1: 0.277, rouge2: 0.1005, rougeL: 0.190
    * BART (pretrained) - rouge1: 0.350, rouge2: 0.153, rougeL: 0.263   
    * BART (normal) - rouge1: 0.284, rouge2: 0.122, rougeL: 0.195   

From these results, I can conclude that BART performs slightly better (but not definitive since it is by margins) than Pegasus but not by much. Better, in this case, is how well the model predicts overlapping n-grams, which is how similar the predicted text (inference) is to the ground truth (human written summary). In other words, BART generates summaries with slightly greater n-gram overlap with ground truth. Essentially, this doesn't really tell us much besides n-gram overlap and since they're pre LLM era, the scores do make sense.

* BERT Scores: For each model:
    * Pegasus (pretrained) - precision: 0.861, recall: 0.879, F1: 0.869
    * Pegasus (normal) - precision: 0.843, recall: 0.867, f1: 0.855
    * BART (pretrained) - Precision: 0.867, recall: 0.880, F1: 0.873
    * BART (normal) - precision: 0.835, recall: 0.879, f1: 0.856

As I remember from above, BERT score measures semantic similarity (through contextual embeddings) rather than surface level n-gram overlaps. In this case, BART is still slightly better showing that BART is slightly better at capturing semantic content from references. Despite these tests, I can conclude that both models are okay at searching keywords but I was unsure about if what the model is producing is factually correct. In this case, I decided to use SummaC, both SummaC-ZS and SummaC-Conv. This would enable us to identify cases when our models contradicts the source, hallucinate details, and misrepresents relationships.   

Since the summac library defaults to using cuda, I have to set it to run on cpu due to my physical limitations of a macbook (since the library does not support MPS yet). For the parameters of the two summa, I set the granularity to be sentence over document since it catches fine-grained errors better. (e.g., one incorrect fact in an otherwise good summary) The model_name parameter I set equal to vitc since vitc is a model that's been trained to detect factual inconsistencies (different from generating summaries.) Other such models include mnli (which is BERT-based) and anli (which is for adversarial datasets). I was deciding on both mnli and vitc but concluded that vitc has the best overall performance and was the model that was used in the original SummaC paper. This was better because vitc uses both textual and visual (attention) features making it effective for this task and since it was pretrained on factuality datasets to classify whether a summary aligns with its source, this was the perfect model for my case. They do take a long time so I would recommend using CUDA or implementing batch processing because it has taken me a long time. This was because SummaC, for each sample, would tokenize the article/summary, perform embedding extraction, and then cross-checking with transformer layers. The library processes samples one by one leading to a long processing time but, generally, calculating the score for BART completed faster.   

Pegasus SummaC scores (average): 
* summac_zs: 0.471, summac_conv: 0.198
BART SummaC scores (average):
* summac_zs: 0.269, summac_conv: 0.195   

Human written summaries, or the ground truth, usually score 0.7-0.8 on CNN/DailyMail so these scores produced by Pegasus and BART indicate that both models perform poorly but Pegasus is relatively better than BART. It further indicates that BART would hallucinate more and BART's summaries have less alignment with the inputted article. Pegasus preserves factual content better but both model struggles with hallucination and this low score could also indicate omissions or paraphrasing mismatches.

**Human Evaluations of both models**   
From what we have seen so far, we can assume that the models perform hallucinations and probably also have some contradictions so we will use human evaluations to sample and check if the model avoids hallucinations/contradictions, prioritize key details (and phrases), summary read naturally, etc. All the inferences generated are fairly fluent since I was able to understand the main point. This means that both models prioritizes key details and provide the essence of each summary while outputting a consistent and fluent summary. Overall, the inferences generated are well-written (with minor grammatical errors or weird phrasing) and concise since each inferences is about 3-5 sentences. From what I can conclude, these models generate more extractive summary rather than abstractive summary since it has been fine-tuned on CNN/DailyMail. This is also because the ground truth (reference summaries) often borrow significant phrasing directly from the articles and evaluation metrics like Rouge reward lexical overlap so models fine-tuned on CNN/DailyMail learn to be more conservative and often copy sentences or phrases directly from the source text. Despite this, we know that the underlying architect of BART and Pegasus is designed for abstractive summarization since Pegasus was pre-trained with an objective called "Gap Sentence Generation" where important sentences were masked and the model was trained to generate them and BART was pre-trained using a denoising autoencoder approach which is designed for text generation tasks. From this, we know that both models generate abstract summaries from their architectural design but, through fine training, produce more inferences that are closer to an extractive summary. 

**Attention Visualization**   
*On the normal model, not pretrained on cnn*   
I decided to further my analysis by using tools like BertViz to compare attention patterns. Attention mechanisms in transformer models determine how the model "focuses" on different parts of the input text (article in this case) when generating each word. I will be able to see which part of the article the model prioritizes, compare architecture (BART (denoising) vs. Pegasus (gap-sentence masking) may attend to different parts of the input), and debug hallucination. I have created several functions to analyze attention and plot it using seaborn.
1. get_attention_full will generate a summary and collect cross-attention (a mechanism that allows one sequence (the query) to focus on relevant information from a different sequence (the key-value pairs).) weights between the encoder and decoder. This function takes in the model, the model's tokenizer, and the article.
    * We would then tokenize the article into model-friendly format (needs to return pytorch tensors)
    * generates the summary using beam search (I was getting this error of GenerateBeamEncoderDecoderOutput which default is not a tensor) and generates a structured output (and not just a tensor)
    * Got another error due to return_dict_in_generate=True so have to check if generate() output is a dict and, if it is, extract the actual token IDs of the generated summary.
    * Then we create an output which runs the full model again but the generated summary is a decoder input (giving my attention weights which .generate() doesn't do).
    * I would then extract the cross-attention layer and convert the raw token IDs back into strings where encoder_tokens: input article tokens and decoder_tokens: output summary tokens.
    * Lastly, just return everything I need for visualizing attention.
2. plot_attention_single_head is the function that I started with which plots a single head in a single layer.
    * I will initially select a layer and make sure to convert it to numpy for plotting (I was also getting an error for not detaching)
    * Call plot attention to plot it
3. plot_attention_avg_heads plots the average attention across all heads in a single layer.
    * same thing as the previous one with slight variation since I am plotting average attention across all heads
4. plot_attention_avg_all plots attention across all layers and heads
    * same thing as the previous 2 just average across all layers and heads
5. plot_attention function draws the actual heatmap using seaborn.
To summarize, X-axis is encoder input tokens (tokens from the original article) and Y-axis is the decoder output tokens (tokens in the generated summary). The heatmap is answering the question of "When the model writes this summary token, how much attention did it pay to this input token?" So, the brighter the cells are the more it reveals how the input token influenced the output token.  

They share the same architecture, but their pre-training strategies are very different, which causes noticeable differences in their cross-attention behavior and as shown in the heatmap. BART is pre-trained with denoising autoencoder (Trained to rebuild the entire sequence, recovering the exact structure and wording) which is why the cross-attention heatmap often shows sharper diagonals meaning output tokens attend to similar-position input tokens. **BART prioritizes reconstruction.** This also means that BART has high attention on matching token positions and works well for paraphrasing and extractive summarization. Pegasus, on the other hand, is pre trained with gap-sentence generation (Trained to summarize or synthesize content that's not directly adjacent) meaning it learns to generate entire missing sentences from the surrounding context resulting in the cross-attention heatmaps looking more diffuse (spread), focusing on salient facts (key words) across the input. **Pegasus prioritizes Gaps / semantic importance.**

I have also outputted the top 3 tokens which get the highest attention weights for the same article. If we want to test more articles, we just change the articles input and see what tokens for each summary token appears the most given BART or Pegasus.   

#### Style of Tokens As Seen
Bart: Strongly localized, diagonal-heavy
* Meaning Bart pays strong attention to aligned input-output positions
* BART was trained to reconstruct sentences, so it learned to rely on local structure.   

Pegasus: More sparse but globally relevant
* Meaning Pegasus pulls from semantically meaningful parts across the article
* More diffuse attention: Pegasus doesn't rely on token-by-token correspondence — instead it jumps to meaningful spans, especially when generating summarizing or abstract tokens
* Fewer clear diagonals attention heap maps which aligns with Pegasus' sentence-level masking pretraining.   

### Part 2: Xsum Dataset    
To complete this analysis, I would also test the two models on the xsum dataset. This will answer the question relating to "out of domain tests" where I have conducted evaluations on a different dataset (xsum) and view if Pegasus' gap-sentence objective generalizes better as previously shown with the cnn/dailymail dataset. Some noteworthy comparison of the two dataset is that cnn/dailymail averages 693 words while xsum averages 377 words. This is close to half the size of cnn/dailymail so it should be able to generate inferences faster and possibly be closer, in similarity, to the ground truth.
* Rouge Scores for each model:
    * Pegasus (pretrained) - rouge1: 0.485, rouge2: 0.263, rougeL: 0.413
    * Pegasus (normal) - rouge1: 0.180, rouge2: 0.028, rougeL: 0.121
    * BART (pretrained) - rouge1: 0.446, rouge2: 0.222, rougeL: 0.373  
    * BART (normal) - rouge1: 0.170, rouge2: 0.035, rougeL: 0.111    
* BERTScores for each model: 
    * Pegasus (pretrained) - precision: 0.926, recall: 0.914, f1: 0.920
    * Pegasus (normal) - precision: 0.839, recall: 0.865, f1: 0.851
    * BART (pretrained) - precision: 0.918, recall: 0.912, f1: 0.915
    * BART (normal) - precision: 0.820, recall: 0.877, f1: 0.848
* SummaC scores for each model (average):
    * Pegasus: summac_zs: 0.603, summac_conv: 0.639
    * BART: summac_zs: 0.321, summac_conv: 0.648

Viewing these scores, we make a similar conclusion to the CNN/DailyMail dataset in that Pegasus outperforms BART slightly. The models perform almost identically regarding lexical overlap and semantic similarity, with tiny margins separating them on different sub-metrics. But regarding SummaC values, if you prioritize the ZS score, Pegasus has a clear advantage in consistency and if you prioritize the Conv score, BART is marginally better.

#### Explanation of Attention
When looking at the heatmap of attention on the Xsum dataset, we see a slight difference compared to the CNN. This is primarily in the BART model since we do not recognize a clear diagonal and this is because of the difference of CNN/DailyMail and Xsum. As mentioned earlier, CNN/DailyMail summaries are generally longer and tend to be more extractive and when a BART model learns from this tendency, it learns this pattern so during generation, the decoder produces a word or phrase for the summary, its cross-attention mechanism will strongly focus on the exact same word or phrase in the encoder's representation of the input article. This results in the clear diagonal line observed in the attention heatmap. On the other hand, Xsum is generally more abstractive in nature, thus making the summaries shorter and capture the main gist of the article using novel wording, not by copying sentences. The BART model will fine tune on Xsum to learn to understand core information of the article and then generate a concise summary from scratch. Because the summary words don't have direct counterparts in the input article, the cross-attention wouldn't show the clear diagonal since it is not word to word. Instead, the decoder attends more broadly to different parts of the input article that contribute to the overall meaning.

### Part 3: Further Exploration
When we're generating inferences, we are using fine-tuned transformers which isn't calculating a loss against a target and backpropagating but rather using its learned weights to predict the next token auto-regressively, searching for the most probable sequence according to what it learned during training/fine-tuning. Regarding adding penalties to the scoring function: this is added when we call model.generate() which allows us to give parameters such as length_penalty (encouraging longer or shorter sequences), repetition_penalty (Discourages the model from repeating the same tokens or phrases), etc. In regards to explicit optimization loop of fine-tuning (calculate loss -> optimize -> backpropagate) for models like BART and PEGASUS that have been fine-tuned for a single, specific sequence-to-sequence task like summarization, the task itself is implicitly encoded in the model's learned parameters through the fine-tuning process. Providing input in the expected format (the article) is sufficient to trigger the learned behavior (generating the summary). This task of fine-tuning becomes implicitly encoded in the model's parameters, meaning simply providing the expected input format (an article) triggers the learned summarization behavior without needing an explicit instruction prompt (unlike general-purpose instruction-tuned LLMs).

For further exploration, I have imported gemini flash 2.0 to critique BART and Pegasus' summaries on Xsum (which would be similar to CNN/DailyMail) based on a few metrics. 
1.  **Factual Consistency:** Identify any statements in the summary that contradict the article or introduce significant information not found in the article. If consistent, state that.
2.  **Completeness:** Identify any key points from the article that are clearly missing from the summary. If nothing crucial seems missing, state that.
3.  **Conciseness:** Identify any parts of the summary that are redundant or unnecessarily wordy relative to the information conveyed. If concise, state that.

The output critique would ideally be textual and highlight specific issues (e.g., factual errors, missing info, redundancy).   

After scanning through 10 examples of these outputs, the summaries are generally more factually correct for Pegasus over BART. In regards to completeness, both BART and Pegasus perform poorly since there is a lot of missing information missing from the summary. Lastly, Pegasus is generally more concise, but not by much. Both models sometimes produce summaries too concise to be called a summary.   

Attempted to generate a refined summary but I ran out of free generations but this would be my next step.   

Generate Refined Summary (S2): Using the same model, Gemini flash 2.0, with the article, S1, and the critique, I would then produce an improved summary S2 which would allow me to evaluate the original summary to the refined summary (using metrics like ROUGE, BERTScore, SummaC, and manual inspection) to develop a clearer case. I would then be able to see if the refinement (the critique outputted by gemini) would improve overall performance of both Pegasus and BART.

### Conclusion
In conclusion, this was my analysis on two transformer models, Pegasus and BART. I have conducted analysis on how they differ in architecture and a thorough analysis on ground truth vs inferences. I, also, have analyzed how these two models' differences have manifested in faithfulness, coherence, and generalization in addition to pair these automated evaluation metrics with human evaluation and behavioral analysis for a clearer picture on what each model brings.      

#### Architectural Inductive Biases
Both models use the encoder-decoder Transformer architecture, but Pegasus is a slightly modified BART so the architecture isn't the key difference, but rather in:   
    1. Training data selection: Pegasus was pretrained on large corpora specifically tuned for summarization   
    2. Sentence-level masking: This creates an inductive bias toward sentence-level abstraction
Pegasus is biased toward semantic understanding across sentences while BART retains token-level alignment and structure awareness.   

#### Regarding faithfulness
BART: More faithful on short inputs, Often copies spans verbatim and useful for factual task
* BART's extractive bias comes from reconstructing real input.    
Pegasus: Sometimes hallucinates if sentence-level semantics aren't clear, reword or synthesize aggressively and better for abstraction (risk errors)
* Pegasus' GSG objective doesn't require recovering exact phrasing.

#### Regarding Coherence
BART: Locally coherent, strong sentence structure, may over repeat structure
* Because Bart's corruption is often local (masks, shuffles, etc), it learns local fluency
Pegasus: Globally coherent, better paragraph-level flow, more natural summarization feel
* Because Pegasus must predict entire disconnected sentences leading to broader discourse-level learning.

#### Regarding Generalization
BART: Good zero-shot performance, pretraining less targeted, adapts well with fine-tuning.    
Pegasus Good zero-shot summarization performance, pretraining tuned for summarization, and already good without fine tuning as shown by our evaluation metrics.
* Because Pegasus' sentence masking aligns more with summarization making it specialized but less flexible for other tasks that are not summarizing.

